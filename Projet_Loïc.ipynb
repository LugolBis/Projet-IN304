{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supression des caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement \n",
    "\n",
    "def SuprCaracterSpeV0(Chaine):  # Prend en entrée une chaine de caractères \n",
    "    ChaineNettoyee = ''\n",
    "    indice = 0\n",
    "    while indice < len(Chaine):\n",
    "        if (Chaine[indice] == 'R') and (indice < len(Chaine)-2) and (Chaine[indice] + Chaine[indice+1] == 'RT'):\n",
    "            indice += 3\n",
    "            while (indice < len(Chaine)-1) and Chaine[indice] != ' ':\n",
    "                indice+=1\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "        elif (Chaine[indice].isalnum() == True):\n",
    "            ChaineNettoyee += Chaine[indice]\n",
    "            indice += 1\n",
    "        elif (Chaine[indice] == \"\\ \"[:-1]) and (indice < len(Chaine)-2) and (Chaine[indice] + Chaine[indice+1] == '\\n'):\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "        elif (Chaine[indice] == '@') and (indice < len(Chaine)-2):\n",
    "            indice += 1\n",
    "            while (indice < len(Chaine)-1) and Chaine[indice] != ' ':\n",
    "                indice += 1\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "        else:\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "    return ChaineNettoyee     # Renvoie la chaine de cractères sans les caractères spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement \n",
    "\n",
    "def SuprCaracterSpe(Chaine):  # Prend en entrée une chaine de caractères \n",
    "    ChaineNettoyee = ''\n",
    "    indice = 0\n",
    "    while indice < len(Chaine):\n",
    "        if (Chaine[indice].isalnum() == True):\n",
    "            ChaineNettoyee += Chaine[indice]\n",
    "            indice += 1\n",
    "        elif (Chaine[indice] == \"\\ \"[:-1]) and (indice < len(Chaine)-2) and (Chaine[indice] + Chaine[indice+1] == '\\n'):\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "        else:\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "    return ChaineNettoyee     # Renvoie la chaine de cractères sans les caractères spéciaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde fonction 'SuprCaracterSpeV0()' ne retire ni les pseudonymes ni les 'RT' des tweets (ce qui est deux fonctionnalitées dont nous avons pris la liberté de rajouter).\n",
    "J'ai comparé sur un petit échantillon, le temps d'éxecution de chaque fonction sur la base de données. Voici les observations notées :\n",
    "\n",
    "En appellant simplement la fonction \n",
    "\n",
    "- OP1 : 9.1s, 6.6s, 6.3s\n",
    "- OP2 : 6.5s, 6.4s, 6.3s \n",
    "\n",
    "- OP1 : 6.5s, 6.5s, 6.4s\n",
    "- OP2 : 6.4s, 6.4s, 6.2s\n",
    "\n",
    "En metant 'DFF = RecupDonneesP(fichier)' \n",
    "\n",
    "- OP1 : 6.3s, 6.4s, 6.1s\n",
    "- OP2 : 6.3s, 6.4s, 6.4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des Données du fichier Json → DataFrame ('DFP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "def ZoneAterrisage(NomFichier):\n",
    "    from json import loads\n",
    "    with open(NomFichier, 'r') as fs, open(r\"C:\\Users\\lddes\\OneDrive\\Bureau\\UVSQ\\L2 Info\\S3\\IN304\\Projet\\Zone d’atterrissage.json\", 'w') as fd:\n",
    "        for ligne in fs:\n",
    "            dicoL = loads(ligne)\n",
    "            Tweet = dicoL[\"TweetText\"]\n",
    "            TweetN = SuprCaracterSpe(Tweet)  # Suprime tous les caractères spéciaux, les RT et les pseudonymes\n",
    "            fd.write(f'{TweetN}\\n')     # On écrit le Tweet nettoyé dans \"Zone d’atterrissage.json\"\n",
    "\n",
    "fichier = r\"C:\\Users\\lddes\\OneDrive\\Bureau\\UVSQ\\L2 Info\\S3\\IN304\\Projet\\aitweets.json\"\n",
    "ZoneAterrisage(fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AuthorLocation</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetLanguage</th>\n",
       "      <th>TweetText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1415291904850153474</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:39Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>@____bruvteresa_ According to research, NASA c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1415291947560828933</td>\n",
       "      <td>Mysore  and  BERLIN</td>\n",
       "      <td>2021-07-14T12:47:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HDataSystems: Artificial Intelligence and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1415291877897605120</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:33Z</td>\n",
       "      <td>246</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @adgpi: Army Technology Board conducted the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1415291886860967940</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @pacorjo: According to a recent survey, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1415291968700264450</td>\n",
       "      <td>Internet</td>\n",
       "      <td>2021-07-14T12:47:54Z</td>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HarbRimah: Making AI Sing https://t.co/FJo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>1421408743770791938</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>2021-07-31T09:53:47Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @innomaticshyd: Top 10 Real world Artificia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>1421424066305605634</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-31T10:54:40Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Iowa State part of U.S. National Science Found...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>1421423882427371521</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>2021-07-31T10:53:57Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @intellimetri: Human Assisted #ArtificialIn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>1421423971858149377</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "      <td>2021-07-31T10:54:18Z</td>\n",
       "      <td>12</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @IainLJBrown: Artificial Intelligence learn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>1421423964845395969</td>\n",
       "      <td>Torremolinos, España</td>\n",
       "      <td>2021-07-31T10:54:16Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @marcusborba: Artificial Intelligence May F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1708 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id        AuthorLocation             CreatedAt  \\\n",
       "0     1415291904850153474                        2021-07-14T12:47:39Z   \n",
       "1     1415291947560828933   Mysore  and  BERLIN  2021-07-14T12:47:49Z   \n",
       "2     1415291877897605120                        2021-07-14T12:47:33Z   \n",
       "3     1415291886860967940                        2021-07-14T12:47:35Z   \n",
       "4     1415291968700264450              Internet  2021-07-14T12:47:54Z   \n",
       "...                   ...                   ...                   ...   \n",
       "1703  1421408743770791938    Liverpool, England  2021-07-31T09:53:47Z   \n",
       "1704  1421424066305605634                        2021-07-31T10:54:40Z   \n",
       "1705  1421423882427371521             127.0.0.1  2021-07-31T10:53:57Z   \n",
       "1706  1421423971858149377  Singapore, Singapore  2021-07-31T10:54:18Z   \n",
       "1707  1421423964845395969  Torremolinos, España  2021-07-31T10:54:16Z   \n",
       "\n",
       "     RetweetCount TweetLanguage  \\\n",
       "0               0            en   \n",
       "1               2            en   \n",
       "2             246            en   \n",
       "3               1            en   \n",
       "4              20            en   \n",
       "...           ...           ...   \n",
       "1703            1            en   \n",
       "1704            0            en   \n",
       "1705           17            en   \n",
       "1706           12            en   \n",
       "1707           17            en   \n",
       "\n",
       "                                              TweetText  \n",
       "0     @____bruvteresa_ According to research, NASA c...  \n",
       "1     RT @HDataSystems: Artificial Intelligence and ...  \n",
       "2     RT @adgpi: Army Technology Board conducted the...  \n",
       "3     RT @pacorjo: According to a recent survey, the...  \n",
       "4     RT @HarbRimah: Making AI Sing https://t.co/FJo...  \n",
       "...                                                 ...  \n",
       "1703  RT @innomaticshyd: Top 10 Real world Artificia...  \n",
       "1704  Iowa State part of U.S. National Science Found...  \n",
       "1705  RT @intellimetri: Human Assisted #ArtificialIn...  \n",
       "1706  RT @IainLJBrown: Artificial Intelligence learn...  \n",
       "1707  RT @marcusborba: Artificial Intelligence May F...  \n",
       "\n",
       "[1708 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "def RecupDonneesP(NomFichier):  # Prend en entier le nom du fichier à analyser\n",
    "    from pandas import DataFrame\n",
    "    from pandas import merge\n",
    "    from json import loads\n",
    "    with open(NomFichier, 'r') as fs, open(r\"C:\\Users\\lddes\\OneDrive\\Bureau\\UVSQ\\L2 Info\\S3\\IN304\\Projet\\Zone d’atterrissage.json\", 'w') as fd:\n",
    "        ligne = fs.readline()\n",
    "        dicoL = loads(ligne)\n",
    "        DFF = DataFrame([dicoL])   # On initialise le DatFrame qui va stocker en local les données de la base\n",
    "        for ligne in fs:\n",
    "            dicoL = loads(ligne)\n",
    "            DF2 = DataFrame([dicoL])\n",
    "            DFF = DFF.merge(DF2, how='outer')  # On ajoute le dictionnaire de chaque ligne au DataFrame                \n",
    "    return DFF\n",
    "\n",
    "fichier = r\"C:\\Users\\lddes\\OneDrive\\Bureau\\UVSQ\\L2 Info\\S3\\IN304\\Projet\\aitweets.json\"\n",
    "DFP = RecupDonneesP(fichier)   # DFP correspond au DataFrame principal, dont les colonnes sont 'id','TweetText', etc\n",
    "DFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opérations d'Analyse\n",
    "Méthode : Récupération des Tweets du DataFrame (pincipal) → Analyse → DataFrame (d'analyse)\n",
    "Les Données d'Analyse seront liées aux données du DataFrame principal par leur index (le numéro de leur ligne tout simplement)\n",
    "\n",
    "Puis pour finir : Fusion des différents DataFrame d'Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les Hashtags ('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etat : Fonctionne parfaitement \n",
    "\n",
    "def Hashtags(chaine): # Entrée : Chaîne de caractères\n",
    "    ListeH = []\n",
    "    SousChaine = chaine.split()\n",
    "    for elem in SousChaine:\n",
    "        if elem.startswith('#') and len(elem)>1:\n",
    "            elem2 = ''\n",
    "            indice = 1\n",
    "            while indice < len(elem):\n",
    "                if (elem[indice] == '_') or (elem[indice].isalnum() == True):\n",
    "                    elem2 += elem[indice]\n",
    "                    indice += 1\n",
    "                else:\n",
    "                    indice = len(elem)\n",
    "            ListeH.append(elem[0]+elem2)\n",
    "    return ListeH   # Renvoie une liste contenant les # du Tweet \n",
    "\n",
    "srt = \"RT @INCANDESClLLA: ⠀ \\n\\n      for a device upgraded to become the greatest artificial intelligence , 𝘽𝙀𝙍𝙏𝙊 squeaked upon the claws threateni…\"\n",
    "Hashtags(srt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DicoHashtags(DF):   # Entrée : DataFrame (la colonne des tweets)\n",
    "    DicoH = {}\n",
    "    from pandas import DataFrame\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "    for i in range(Taille[0]):\n",
    "        Tweet = DF.loc[i]\n",
    "        ListeHashtags = Hashtags(Tweet)\n",
    "        for h in ListeHashtags:\n",
    "            if h in DicoH:\n",
    "                nombre = DicoH[h]\n",
    "                nombre +=1\n",
    "                DicoH[h] = nombre\n",
    "            else:\n",
    "                DicoH[h] = 1\n",
    "    del DicoH['#']    # Il y a 33 # incomplets dans la base de données, ils sont pris en compte par la fonction, on prend donc le soin de suprimer cette clé du dico           \n",
    "    return DicoH      # Renvoie un Dictionnaire contenant tous les Hashtags en clé et le nombre de fois qu'ils sont tweeté en valeur\n",
    "\n",
    "DicoH = DicoHashtags(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "def DFHashtags(DF): # En entrée un DataFrame contenant la colonne des \"TweetText\"\n",
    "    from pandas import DataFrame\n",
    "    from pandas import concat\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "    i = 0\n",
    "    Tweet = DF.loc[i]\n",
    "    ListeHashtags = Hashtags(Tweet)\n",
    "    DFA = DataFrame({'#':[ListeHashtags]}, index=[i])\n",
    "    for i in range(1,Taille[0]):\n",
    "        Tweet = DF.loc[i]\n",
    "        ListeHashtags = Hashtags(Tweet)\n",
    "        DFB = DataFrame({'#':[ListeHashtags]}, index=[i])\n",
    "        DFC = concat([DFA, DFB])\n",
    "        DFA = DFC\n",
    "    return DFA\n",
    "\n",
    "DFh = DFHashtags(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les Arobases @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement \n",
    "\n",
    "def Arobases(chaine): # Prend en entrée un str\n",
    "    ListeA = []\n",
    "    SousChaine = chaine.split()\n",
    "    for elem in SousChaine:\n",
    "        if elem.startswith('@') and len(elem)>1:\n",
    "            elem2 = ''\n",
    "            indice = 1\n",
    "            while indice < len(elem):\n",
    "                if (elem[indice] == '_') or (elem[indice].isalnum() == True):\n",
    "                    elem2 += elem[indice]\n",
    "                    indice += 1\n",
    "                else:\n",
    "                    indice = len(elem)\n",
    "            ListeA.append(elem[0]+elem2)\n",
    "    return ListeA   # Renvoie une liste contenant les @ du Tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DicoArobases(DF):   # Entrée : DataFrame (la colonne des tweets)\n",
    "    DicoA = {}\n",
    "    from pandas import DataFrame\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "    for i in range(Taille[0]):\n",
    "        Tweet = DF.loc[i]\n",
    "        ListeArobases = Arobases(Tweet)\n",
    "        for h in ListeArobases:\n",
    "            if h in DicoA:\n",
    "                nombre = DicoA[h]\n",
    "                nombre +=1\n",
    "                DicoA[h] = nombre\n",
    "            else:\n",
    "                DicoA[h] = 1 \n",
    "    del DicoA['@']               \n",
    "    return DicoA      # Renvoie un Dictionnaire contenant tous les Hashtags en clé et le nombre de fois qu'ils sont tweeté en valeur\n",
    "\n",
    "DicoA = DicoArobases(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "def DFArobases(DF): # En entrée un DataFrame contenant la colonne des \"TweetText\"\n",
    "    from pandas import DataFrame\n",
    "    from pandas import concat\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "    i = 0\n",
    "    Tweet = DF.loc[i]\n",
    "    ListeArobases = Arobases(Tweet)\n",
    "    DFA = DataFrame({'@':[ListeArobases]}, index=[i])\n",
    "    for i in range(1,Taille[0]):\n",
    "        Tweet = DF.loc[i]\n",
    "        ListeArobases = Arobases(Tweet)\n",
    "        DFB = DataFrame({'@':[ListeArobases]}, index=[i])\n",
    "        DFC = concat([DFA, DFB])\n",
    "        DFA = DFC\n",
    "    return DFA\n",
    "\n",
    "DFa = DFArobases(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame d'Analyse \n",
    "Ce DataFrame contient toute les Analyses PAR Tweet, il permet de récupérer grâce à l'index de la ligne du Tweet (stocké dans le DataFrame principal : 'DFP')\n",
    "les données d'analyse correspondant au Tweet (les @ mentionnés, les # mentionnés, les Topiocs mentionnés, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Méthode n°1\n",
    "\n",
    "On parcour de manière itérative le Dataframe en fonction de l'index (le numéro de ligne) et on ajoute les opérations d'analyses dans un dictionnaire.\n",
    "Puis une fois toutes les lignes analysées ont crée un DataFrame à partir du dictionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "def DataFrameAnalyse(DF):   # En entrée un DataFrame contenant la colonne des \"TweetText\"\n",
    "    from pandas import DataFrame\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "    dicoAnalyses = {}\n",
    "    i = 0\n",
    "    Tweet = DF.loc[i]\n",
    "    ListeArobases = Arobases(Tweet)\n",
    "    dicoAnalyses['@'] = [ListeArobases]\n",
    "    ListeHashtags = Hashtags(Tweet)\n",
    "    dicoAnalyses['#'] = [ListeHashtags]\n",
    "    for i in range(1,Taille[0]):\n",
    "        Tweet = DF.loc[i]\n",
    "        ListeArobases = Arobases(Tweet)\n",
    "        dicoAnalyses['@'].append(ListeArobases)\n",
    "        ListeHashtags = Hashtags(Tweet)\n",
    "        dicoAnalyses['#'].append(ListeHashtags)\n",
    "    DFA = DataFrame(dicoAnalyses)\n",
    "    return DFA\n",
    "\n",
    "DFA = DataFrameAnalyse(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Méthode n°2\n",
    "\n",
    "On reprend les Dataframe créer pour chaque Analyses (donc des DataFrame d'une seule colonne) et on les fusionne à l'aide de la fonction concat()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "DFA2 = concat([DFa, DFh], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctionnalitées Suplémentaires :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actualisation manuelle des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Etat : Fonctionne parfaitement\n",
    "\n",
    "def Refresh():\n",
    "    fichier = r\"C:\\Users\\lddes\\OneDrive\\Bureau\\UVSQ\\L2 Info\\S3\\IN304\\Projet\\aitweets.json\"\n",
    "    ZoneAterrisage(fichier)\n",
    "\n",
    "Refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le nombre de publications par utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1418403326245306375', '4')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NbPostUtilisateur(ID) : # En Entrée : 'str' (l'ID de l'utilisateur)\n",
    "    global DFP\n",
    "    Taille = DFP.shape\n",
    "    for i in range(Taille[0]):\n",
    "        if DFP[\"id\"].loc[i] == ID:\n",
    "            return (ID, DFP[\"RetweetCount\"].loc[i])  # En sortie : 'tuple' (ID, nombre de Publications)\n",
    "\n",
    "NbPostUtilisateur(\"1418403326245306375\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le nombre de publications par hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('#AI', 204)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def NbPostHashtags(hashtag): # En entrée : 'str'\n",
    "    global DicoH\n",
    "    if hashtag in DicoH:\n",
    "        return (hashtag, DicoH[hashtag])\n",
    "    else:\n",
    "        return (hashtag, 0) # En sortie : 'tuple' (#, nombre de publications dans lequel il est mentionné)\n",
    "    \n",
    "NbPostHashtags('#AI')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top K hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Nombre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#ArtificialIntelligence</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#AI</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#MachineLearning</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#ai</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#DataScience</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>#artificialintelligence</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>#BigData</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>#learning</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>#ML</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>#5G</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           #  Nombre\n",
       "11   #ArtificialIntelligence     207\n",
       "5                        #AI     204\n",
       "2           #MachineLearning      86\n",
       "27                       #ai      52\n",
       "3               #DataScience      50\n",
       "10   #artificialintelligence      44\n",
       "49                  #BigData      25\n",
       "16                 #learning      24\n",
       "83                       #ML      23\n",
       "167                      #5G      21"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etat : Fonctionne IMparfaitement !!!!\n",
    "\n",
    "def TopKH(K):\n",
    "    from pandas import DataFrame\n",
    "    global DicoH\n",
    "    DFKH = DataFrame(DicoH.items(), columns=['#', 'Nombre'])\n",
    "    DFKH = DFKH.sort_values(by='Nombre', ascending=False)  # On Tri le Dataframe selon les valeurs de la deuxième colonne 'Nombre' et on precise 'ascending=False' pour l'ordre décroissant\n",
    "    return DFKH.head(K)\n",
    "\n",
    "TopKH(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top K utilisateurs mentionnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@</th>\n",
       "      <th>Nombre de Mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@IainLJBrown</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@Paula_Piccard</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@nigewillson</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>@machinelearnTec</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>@akbarth3great</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>@JoshuaBarbeau</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>@sankrant</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>@SpirosMargaris</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@machinelearnflx</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>@Datascience__</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    @  Nombre de Mentions\n",
       "26       @IainLJBrown                 112\n",
       "55     @Paula_Piccard                  49\n",
       "18       @nigewillson                  24\n",
       "36   @machinelearnTec                  17\n",
       "284    @akbarth3great                  17\n",
       "680    @JoshuaBarbeau                  17\n",
       "978         @sankrant                  17\n",
       "147   @SpirosMargaris                  16\n",
       "27   @machinelearnflx                  16\n",
       "44     @Datascience__                  15"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Etat : Fonctionne IMparfaitement !!!!\n",
    "\n",
    "def TopKA(K):\n",
    "    from pandas import DataFrame\n",
    "    global DicoA\n",
    "    DFKA = DataFrame(DicoA.items(), columns=['@', 'Nombre de Mentions'])\n",
    "    DFKA = DFKA.sort_values(by='Nombre de Mentions', ascending=False) # On Tri le Dataframe selon les valeurs de la deuxième colonne 'Nombre' et on precise 'ascending=False' pour l'ordre décroissant\n",
    "    return DFKA.head(K)\n",
    "\n",
    "TopKA(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problèmes !!!\n",
    "\n",
    "- La complexitée de la supression des caractères spéciaux des tweets pour la Zone d'atterrissage. Le problème étant que la méthode d'encodage 'utf-8' ne fonctionne pas, ce qui nous force à parcourir deux fois les chaînes de caractères de la base de données (le fichier Json contenant tous les tweets)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
