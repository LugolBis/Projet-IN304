{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditions d'utilisation \n",
    "Pour que **InPoDa** fonctionne correctement vérifiez que les conditions suivantes sont réunies :\n",
    "- Installation des modules : ```Pandas```, ```Textblob```, (```Spacy```?, ```Deep_translator```?, ```Gradio```?)\n",
    "- Le fichier ***\"aitweets.json\"*** doit être dans le même dossier que  ce notebook \"**InPoDa POO Desmares-Desfontaines**\"\n",
    "- Mise en garde : l'éxécution du script à été soigneusement pensé et ordonné. L'éxécution d'une fonction une seconde fois risque de mener à une erreur car le type des variables manipulées aura changer entre temps. En effet les variables principales (**DicoA**, **DicoH**, **DFP**), sont converties en objet **Inpoda** vers la fin du script. Elles ne sont donc plus compatibles à certaines opérations (comme .keys() par exemple) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation des modules\n",
    "Pour commencer on s'assure que tous les modules nécessaires à InPoDa sont correctement installés.\n",
    "On s'assure donc de l'installation des modules : ```Pandas```, ```Textblob```, (```Spacy```?, ```Deep_translator```?), ```Gradio```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lddes\\miniconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: textblob in c:\\users\\lddes\\miniconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Requirement already satisfied: spacy in c:\\users\\lddes\\miniconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: gradio in c:\\users\\lddes\\miniconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: numpy~=1.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (1.23.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: requests~=2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (2.28.1)\n",
      "Requirement already satisfied: pydub in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.3.1)\n",
      "Requirement already satisfied: pydantic>=2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (2.5.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.14.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.19.4)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.7.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.24.0.post1)\n",
      "Requirement already satisfied: packaging in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (23.1)\n",
      "Requirement already satisfied: fastapi in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.104.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (5.1.2)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (3.9.10)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (4.8.0)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: python-multipart in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: typer[all]<1.0,>=0.9 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (0.9.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio) (2.1.1)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio-client==0.7.0->gradio) (2023.10.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from gradio-client==0.7.0->gradio) (11.0.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.14.0->gradio) (4.64.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (2.14.5)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests~=2.0->gradio) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests~=2.0->gradio) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests~=2.0->gradio) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from requests~=2.0->gradio) (2.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from fastapi->gradio) (0.27.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from fastapi->gradio) (3.7.1)\n",
      "Requirement already satisfied: httpcore in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from httpx->gradio) (1.0.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from httpx->gradio) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio) (1.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.31.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.15.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lddes\\miniconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas  \n",
    "!pip install textblob\n",
    "!pip install spacy\n",
    "## modules pour les Topics\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supression des caractères spéciaux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de correctement suprimer les caractères spéciaux de tous les tweets de la base de donnée on commence par suprimer les caractères spéciaux. \n",
    "De plus dans le cas spécifique de la base de données de tweets on utilise des expressions régulière afin de nettoyer au maximum les tweets et d'éviter tout désagrément pour les analyses qui seront réalisées par la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SuprCaracterSpe(Chaine=str):  # Entrée : 'str' (le tweet brut)\n",
    "# On suprime les caractères spéciaux et éléments suprflus du tweet\n",
    "\n",
    "    ChaineNettoyee = ''\n",
    "    indice = 0\n",
    "\n",
    "    while indice < len(Chaine):\n",
    "        if (Chaine[indice] == 'R') and (indice < len(Chaine)-2) and (Chaine[indice] + Chaine[indice+1] == 'RT'): # On suprime les \"RT\"\n",
    "            indice += 3\n",
    "            while (indice < len(Chaine)-1) and Chaine[indice] != ' ':\n",
    "                indice+=1\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "        \n",
    "        elif (Chaine[indice] == 'h') and (indice < len(Chaine)-2) and (Chaine[indice] + Chaine[indice+1:indice+4] == 'http'): # On suprime les URL\n",
    "            indice += 4\n",
    "            while (indice < len(Chaine)-1) and Chaine[indice] != ' ':\n",
    "                indice+=1\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "\n",
    "        elif (Chaine[indice].isalnum() == True):\n",
    "            ChaineNettoyee += Chaine[indice]\n",
    "            indice += 1\n",
    "\n",
    "        elif (Chaine[indice] == \"\\ \"[:-1]) and (indice < len(Chaine)-2) and (Chaine[indice] + Chaine[indice+1] == '\\n'): # On suprime les sauts de ligne\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "\n",
    "        elif (Chaine[indice] == '@') and (indice < len(Chaine)-2):  # On suprime les pseudonymes (donc les arobases) \n",
    "            indice += 1\n",
    "            while (indice < len(Chaine)-1) and Chaine[indice] != ' ':\n",
    "                indice += 1\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "            \n",
    "        else:\n",
    "            ChaineNettoyee += ' '\n",
    "            indice += 1\n",
    "    return ChaineNettoyee     # Sortie : 'str' (Le tweet sans caractère spéciaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "°Cette section est terminée mais pour des raisons de complexitée technique elle n'est pas connectée au reste°\n",
    "°En effet l'analyse des sujets d'un seul tweet prend environ 3 secondes... ce qui est énorme à l'échechelle de la base de données. Les fonctions relatives aux Topics sont donc fonctionnelles mais bien trop longues.°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Traduction(text):  # Entrée : 'str'\n",
    "    # On traduit le text\n",
    "    from deep_translator import GoogleTranslator\n",
    "    traduction = GoogleTranslator(source = 'auto', target='en').translate(text)\n",
    "    text = traduction\n",
    "    return(text)     # Sortie : 'str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_sujet_tweet(text): #prends en argument un tweet nettoyé de tous caractères spéciaux\n",
    "    from spacy import load\n",
    "    # Cette première partie permet de faire une analyse du sujet, fonctionelle pour toutes les langues.\n",
    "    # Si le tweet est écrit dans une autre langue que l'anglais, on le traduit d'abord pour rendre l'analyse fonctionelle.\n",
    "    #langue = detect(text)     \n",
    "\n",
    "    #traduction = GoogleTranslator(source = 'auto', target='en').translate(text)\n",
    "    #text = traduction\n",
    "    #print(text)\n",
    "\n",
    "    nlp = load('en_core_web_sm')       # creation d'un objet spacy\n",
    "    doc = nlp(text)                          # rends le texte utilisable pour l'ordinateur\n",
    "    sujets = []\n",
    "\n",
    "    for sent in doc.sents:                   # iterate over the sentences in the text\n",
    "        for ent in sent.ents:                # iterate over the named entities in the sentence\n",
    "            sujets.append(sent.text)          # add the entity to the list of main topics\n",
    "\n",
    "            #if \"artificial intelligence\" in ent:\n",
    "                #sujets.append('artificial intelligence')\n",
    "\n",
    "    return sujets # Sortie : 'list' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topics(text): # Entrée : 'str'\n",
    "    # Topics() analyse et extrait les sujets du tweet \n",
    "    return analyse_sujet_tweet(Traduction(text)) # Sortie : 'list'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la classe ***Tweet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    # Cette classe permet d'encapsuler toutes les fonctions d'analyses appliquée à un seul tweet.\n",
    "    # Le but est ici de pouvoir aisément extraire les données du tweet. \n",
    "\n",
    "    def __init__(self, chaine='TweetParDéfaut') :\n",
    "        # 'chaine' doit être un 'str'\n",
    "        self.chaine = chaine # Reconversion du Tweet en 'str'\n",
    "\n",
    "        def Hashtags(chaine=str): # Entrée : 'str'\n",
    "            # Hashtags() extrait la liste des hashtags du tweet en entrée   \n",
    "            ListeH = []\n",
    "            SousChaine = chaine.split()\n",
    "            \n",
    "            for elem in SousChaine:\n",
    "                if elem.startswith('#') and len(elem)>1:\n",
    "                    elem2 = ''\n",
    "                    indice = 1\n",
    "                    while indice < len(elem):\n",
    "                        if (elem[indice] == '_') or (elem[indice].isalnum() == True):\n",
    "                            elem2 += elem[indice]\n",
    "                            indice += 1\n",
    "                        else:\n",
    "                            indice = len(elem)\n",
    "                    ListeH.append(elem[0]+elem2)\n",
    "            return ListeH   # Sortie : 'list' contenant les '#' du tweet\n",
    "        \n",
    "        self.Hashtags = Hashtags(chaine)\n",
    "\n",
    "        def Arobases(chaine=str): # Entrée : 'str'\n",
    "            # Arobases() extrait la liste des arobases du tweet en entrée\n",
    "            ListeA = []\n",
    "            SousChaine = chaine.split()\n",
    "\n",
    "            for elem in SousChaine:\n",
    "                if elem.startswith('@') and len(elem)>1:\n",
    "                    elem2 = ''\n",
    "                    indice = 1\n",
    "                    while indice < len(elem):\n",
    "                        if (elem[indice] == '_') or (elem[indice].isalnum() == True):\n",
    "                            elem2 += elem[indice]\n",
    "                            indice += 1\n",
    "                        else:\n",
    "                            indice = len(elem)\n",
    "                    ListeA.append(elem[0]+elem2)\n",
    "            return ListeA   # Sortie : 'list' contenant les @ du tweet \n",
    "\n",
    "        self.Arobases = Arobases(chaine)\n",
    "\n",
    "        def Sentiment(Tweet=str): # Entrée : 'str'\n",
    "            # Sentiment() extrait le sentiment du tweet en entrée\n",
    "            from textblob import TextBlob\n",
    "            blob = TextBlob(Tweet)\n",
    "            polarite = blob.sentiment.polarity\n",
    "\n",
    "            if polarite > 0:\n",
    "                return 'Positif'\n",
    "            elif polarite < 0:\n",
    "                return 'Negatif'\n",
    "            else:\n",
    "                return 'Neutre' # Sortie : 'str' contenant le sentiment du tweet\n",
    "            \n",
    "        self.Sentiment = Sentiment(chaine)\n",
    "    \n",
    "    def __repr__(self): # On définit ici la représentation de notre objet de classe \"Tweet\"\n",
    "        return self.chaine # Sa représentation est sous forme de 'str'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la classe ***Inpoda***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inpoda:\n",
    "    # Cette classe permet d'encapsuler toutes les fonctions d'analyses appliquée à la base de données.\n",
    "    # Le but est ici de pouvoir aisément extraire les données du DataFrame principal (DFP) et de nos deux dictionnaires (DicoA et DicoH). \n",
    "\n",
    "    def __init__(self, Dico=None, DF=None):\n",
    "        if DF is None:    # Si l'objet converti en 'Inpoda' n'est pas un 'DataFrame'\n",
    "            DF = DataFrame([{\"DataFrameParDéfaut\": True}])  # On crée un DataFrame par défaut\n",
    "        elif Dico is None: # Si l'objet converti en 'Inpoda' n'est pas un 'dict'\n",
    "            Dico = {\"DicoParDéfaut\": True}   # On crée un Dictionnaire par défaut\n",
    "\n",
    "        self.Dico = Dico\n",
    "        assert type(self.Dico) == type({})   # Avant d'aller plus loin on vérifie qu'il n'y a pas d'erreur de type d'objet\n",
    "        self.DF = DF\n",
    "        assert type(self.DF) == type(DataFrame())   # Avant d'aller plus loin on vérifie qu'il n'y a pas d'erreur de type d'objet\n",
    "\n",
    "\n",
    "    def TopKH(self, K=5, DicoH=None):# Entrée : {K : 'int' (si K>0 on aura un tri décroissant, si K<0 on aura un tri croissant)\n",
    "                                            #    DicoH : 'dict' (Dictionnaire contenant tous les Hashtags de la base de données et leur nombre)}\n",
    "        # TopKH() élabore un DataFrame contenant le top K des hashtags \n",
    "        from pandas import DataFrame\n",
    "        if (DicoH is None) or (type(DicoH)==Inpoda):   # Si le Dictionnaire n'est pas du bon type on le converti en 'dict'\n",
    "            DicoH = self.Dico\n",
    "        \n",
    "        DFKH = DataFrame(DicoH.items(), columns=['#', 'Nombre de Tweets'])\n",
    "        if K > 0:\n",
    "            DFKH = DFKH.sort_values(by='Nombre de Tweets', ascending=False) # On tri le Dataframe selon les valeurs de la deuxième colonne 'Nombre de Tweets' et on precise 'ascending=False' pour l'ordre décroissant\n",
    "        else:\n",
    "            DFKH = DFKH.sort_values(by='Nombre de Tweets', ascending=True) # Inversement on tri le DataFrame par ordre Décroissant\n",
    "        return DFKH.head(abs(K)) # On passe la valeur absolue de K dans la méthode .head() pour s'assurer qu'elle affiche correctement le Top demandé\n",
    "                # Sortie : 'DataFrame' (de deux colonnes contenant les Utilisateurs mentionnés et le Nombre de fois qu'ils ont été mentionné)\n",
    "\n",
    "\n",
    "    def TopKU(self, K=5, DFP=None):   # Entrée : {K : 'int' (si K>0 on aura un tri décroissant, si K<0 on aura un tri croissant)\n",
    "                                     #          DFP : 'DataFrame' (Le DataFrame principal)}\n",
    "        # TopKU() élabore un DataFrame contenant le top K des utilisateurs qui postent le plus\n",
    "        from pandas import concat\n",
    "        if (DFP is None) or (type(DFP)==Inpoda):  # Si le DataFrame n'est pas du bon type on le converti en 'DataFrame'\n",
    "            DFP = self.DF\n",
    "\n",
    "        Taille = DFP[\"RetweetCount\"].shape  # On récupère les dimensions de la colonne \"RetweetCount\" de DFP \n",
    "        for i in range(Taille[0]):\n",
    "            valeur = int(DFP[\"RetweetCount\"].loc[i])  # On converti les 'str' de la colonne \"RetweetCount\" en entier pour correctement les trier\n",
    "            DFP[\"RetweetCount\"].loc[i] = valeur       \n",
    "        \n",
    "        DFKU = concat([DFP[\"Utilisateurs\"], DFP[\"RetweetCount\"]], axis=1)\n",
    "        if K > 0:\n",
    "            DFKU = DFKU.sort_values(by=\"RetweetCount\", ascending=False) # On tri le Dataframe selon les valeurs de la deuxième colonne 'RetweetCount' et on precise 'ascending=False' pour l'ordre décroissant\n",
    "        else:\n",
    "            DFKU = DFKU.sort_values(by=\"RetweetCount\", ascending=True) # Inversement on tri le DataFrame par ordre Décroissant\n",
    "        return DFKU.head(abs(K))  # On passe la valeur absolue de K dans la méthode .head() pour s'assurer qu'elle affiche correctement le Top demandé\n",
    "         # Sortie : 'DataFrame' (de deux colonnes contenant les Utilisateurs et le Nombre de fois qu'ils ont tweeté) \n",
    "\n",
    "\n",
    "    def TopKA(self, K=5, DicoA=None):  # Entrée : {K :'int' (si K>0 on aura un tri décroissant, si K<0 on aura un tri croissant)\n",
    "                        #          DicoA : 'dict' (Dictionnaire contenant tous les Arobases de la base de données et leur nombre)}\n",
    "        # TopKA() élabore un DataFrame contenant le top K des utilisateurs les plus mentionnés\n",
    "        from pandas import DataFrame\n",
    "        if (DicoA is None) or (type(DicoA)==Inpoda):   # Si le Dictionnaire n'est pas du bon type on le converti en 'dict'\n",
    "            DicoA = self.Dico\n",
    "\n",
    "        DFKA = DataFrame(DicoA.items(), columns=['@', 'Nombre de Mentions'])\n",
    "        if K > 0:\n",
    "            DFKA = DFKA.sort_values(by='Nombre de Mentions', ascending=False) # On tri le Dataframe selon les valeurs de la deuxième colonne 'Nombre de Mentions' et on precise 'ascending=False' pour l'ordre décroissant\n",
    "        else:\n",
    "            DFKA = DFKA.sort_values(by='Nombre de Mentions', ascending=True) # Inversement on tri le DataFrame par ordre Décroissant  \n",
    "        return DFKA.head(abs(K))  # On passe la valeur absolue de K dans la méthode .head() pour s'assurer qu'elle affiche correctement le Top demandé\n",
    "            # Sortie : 'DataFrame' (de deux colonnes contenant les Utilisateurs et le Nombre de fois qu'ils ont été mentionnés) \n",
    "\n",
    "    def NbPostUtilisateur(self, Utilisateur=\"Lugol\", DFP=None) : # Entrée : {Utilisateur : 'str' (Le Nom de l'utilisateur)\n",
    "                                                                  #          DFP : 'DataFrame' (DataFrame principal)}\n",
    "        # NbPostUtilisateur() extrait le nombre de tweets d'un Utilisateur \n",
    "        if (DFP is None) or (type(DFP)==Inpoda):  # Si le DataFrame n'est pas du bon type on le converti en 'DataFrame'\n",
    "            DFP = self.DF\n",
    "\n",
    "        Taille = DFP.shape\n",
    "        for i in range(Taille[0]):\n",
    "            if DFP[\"Utilisateurs\"].loc[i] == Utilisateur:\n",
    "                return (Utilisateur, DFP[\"RetweetCount\"].loc[i])  \n",
    "        return (Utilisateur, 0)   # Sortie : 'tuple' (Utilisateur, nombre de Publications)\n",
    "\n",
    "\n",
    "    def NbPostHashtags(self, hashtag='#ParDéfaut', DicoH=None): # Entrée : { hashtag : 'str' (Le Hashtag recherché)\n",
    "                                                                #            DicoH : 'dict' (Le dictionnaire des Hastags)}\n",
    "        # NbPostHashtags() extrait le nombre de tweets mentionnant un hashtag  \n",
    "        if (DicoH is None) or (type(DicoH)==Inpoda):   # Si le Dictionnaire n'est pas du bon type on le converti en 'dict'\n",
    "            DicoH = self.Dico\n",
    "\n",
    "        if hashtag in DicoH:\n",
    "            return (hashtag, DicoH[hashtag])\n",
    "        else:\n",
    "            return (hashtag, 0) # Sortie : 'tuple' (#, nombre de publications dans lequel il est mentionné)\n",
    "    \n",
    "\n",
    "    def TweetsUtilisateur(self, Utilisateur=\"Lugol\", DFP=None): # Entrée : { Utilisateur : 'str' (Nom de l'Utilisateur)\n",
    "                                                                #            DFP : 'DataFrame' (DataFrame principal) }\n",
    "        # TweetsUtilisateur() élabore un DataFrame contenant tous les tweets de l'Utilisateur \n",
    "        from pandas import DataFrame\n",
    "        if (DFP is None) or (type(DFP)==Inpoda):  # Si le DataFrame n'est pas du bon type on le converti en 'DataFrame'\n",
    "            DFP = self.DF\n",
    "        if Utilisateur[0] != \"@\":\n",
    "            Utilisateur = \"@\" + Utilisateur\n",
    "        \n",
    "        ExpressionR = 'RT ' + Utilisateur  # On utilise une expression régulière pour savoir si le tweet est un \"Retweet ('RT')\"\n",
    "        ListeTweets = []\n",
    "        Taille = DFP[\"TweetText\"].shape\n",
    "        for indice in range(Taille[0]):\n",
    "            tweet = DFP[\"TweetText\"].loc[indice]\n",
    "            tweet = tweet.chaine   # On accède au tweet sous forme de 'str' avec l'attribut .chaine de la classe Tweet\n",
    "            if (ExpressionR in tweet) and (tweet[len(ExpressionR)+2:] not in ListeTweets):\n",
    "                ListeTweets.append(tweet[len(ExpressionR)+2:])\n",
    "            elif (DFP[\"Utilisateurs\"].loc[indice] == Utilisateur):\n",
    "                ListeTweets.append(tweet)\n",
    "\n",
    "        if ListeTweets == []:\n",
    "            return DataFrame([{\"Tweets postés par : \" + Utilisateur:\"Erreur : Aucun Tweet posté\"}])\n",
    "        else:\n",
    "            return DataFrame({\"Tweets postés par : \" + Utilisateur:ListeTweets}) \n",
    "                # Sortie : 'DataFrame' contenant les à chaque ligne un tweet de l'utilisateur (Tweet brut) \n",
    "                #          (attention l'index des Tweets de DFTU ne correspondent pas à leur index dans DFP)\n",
    "\n",
    "\n",
    "    def TweetsMentionUtilisateur(self, Utilisateur = \"Lugol\", DFP = None): # Entrée : { Utilisateur : 'str' (Nom d'Utilisateur)\n",
    "                                                                           #            DFP : 'DataFrame' (DataFrame principal)\n",
    "        # TweetsMentionUtilisateur() élabore un DataFrame contenant les tweets mentionnant un utilisateur\n",
    "        from pandas import DataFrame\n",
    "        if (DFP is None) or (type(DFP)==Inpoda):  # Si le DataFrame n'est pas du bon type on le converti en 'DataFrame'\n",
    "            DFP = self.DF\n",
    "\n",
    "        Taille = DFP.shape\n",
    "        ListeIndiceTweets = []\n",
    "        for i in range(Taille[0]):\n",
    "            ListeA = DFP[\"@\"].loc[i]\n",
    "            if Utilisateur in ListeA:\n",
    "                ListeIndiceTweets.append(i)\n",
    "\n",
    "        DFTMU = DataFrame(DFP[\"TweetText\"].iloc[ListeIndiceTweets])\n",
    "        DFTMU = DFTMU.rename(columns={\"TweetText\":\"Tweets mentionnant : \" + Utilisateur})  # On renomme la colonne pour plus de clarté \n",
    "        return DFTMU     # Sortie : 'DataFrame' contenant à chaque ligne les Tweets mentionnant l'Utilisateur, \n",
    "                                # l'index de chaque ligne correpond à celui du Tweet brut stocké dans DFP \n",
    "\n",
    "\n",
    "    def UtilisateurMentionHashtag(self, Hashtag = \"#ParDéfaut\", DFP = None): # Entrée : { Hastag : 'str' (Hashtag)\n",
    "                                             #            DFP : 'DataFrame' (DataFrame principal) }\n",
    "        # UtilisateruMentionHashtag() élabore un DataFrame contenant les Utilisateurs qui ont mentionné le Hashtag dans leur tweet\n",
    "        from pandas import DataFrame\n",
    "        if (DFP is None) or (type(DFP)==Inpoda):  # Si le DataFrame n'est pas du bon type on le converti en 'DataFrame'\n",
    "            DFP = self.DF\n",
    "\n",
    "        Taille = DFP.shape\n",
    "        ListeIndiceTweets = []\n",
    "        for i in range(Taille[0]):\n",
    "            ListeH = DFP[\"#\"].loc[i]\n",
    "            if Hashtag in ListeH:\n",
    "                ListeIndiceTweets.append(i)\n",
    "\n",
    "        if ListeIndiceTweets == []:\n",
    "            return DataFrame([{\"Utilisateurs\":\"Erreur : Votre Hashtag n'existe pas\"}])\n",
    "        else:\n",
    "            return DataFrame(DFP[\"Utilisateurs\"].iloc[ListeIndiceTweets]) \n",
    "              # Sortie : 'DataFrame' contenant à chaque ligne un Utilisateur qui à mentionner le Hashtag dans son tweet, \n",
    "              #          l'index correspond à celui de l'Utilisateur dans DFP\n",
    "\n",
    "\n",
    "    def UtilisateurMentionUtilsateur(self, Utilisateur = \"Lugol\", DFP = None):  # Entrée : { Utilisateur : 'str' (Nom d'Utilisateur)\n",
    "                                                                    #            DFP : 'DataFrame' (DataFrame principal)\n",
    "        # UtilisateruMentionUtilisateur() élabore un DataFrame contenant les Utilisateurs mentionnés par un Utilisateur spécifique\n",
    "        from pandas import DataFrame\n",
    "        if (DFP is None) or (type(DFP)==Inpoda):  # Si le DataFrame n'est pas du bon type on le converti en 'DataFrame'\n",
    "            DFP = self.DF\n",
    "            \n",
    "        Taille = DFP.shape\n",
    "        for i in range(Taille[0]):\n",
    "            if DFP[\"Utilisateurs\"].loc[i] == Utilisateur:\n",
    "                return DataFrame({Utilisateur + \" a mentionné :\":DFP[\"@\"].loc[i]})   \n",
    "        return DataFrame([{\"Utilisateurs\":\"Erreur : Votre Utilisateur n'existe pas\"}]) \n",
    "            # Sortie : 'DataFrame'\n",
    "\n",
    "\n",
    "    def __repr__(self):     \n",
    "        if (self.Dico != {\"DicoParDéfaut\": True}) and (self.Dico != {}):  # On adapte la représentation de notre objet en fonction de son type initial \n",
    "            return repr(self.Dico)\n",
    "        else:\n",
    "            return repr(self.DF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des Tweets du fichier Json → Json (Zone d'aterrissage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZoneAtterrissage(NomFichier=str):\n",
    "    # On écrit tous les tweets nettoyés dans un fichier json \n",
    "    from json import loads\n",
    "\n",
    "    with open(NomFichier, 'r') as fs, open(\"Zone d’atterrissage.json\", 'w') as fd:\n",
    "        ListeTweets = []\n",
    "        for ligne in fs:\n",
    "            dicoL = loads(ligne)\n",
    "            tweet = dicoL[\"TweetText\"] \n",
    "            tweetN = SuprCaracterSpe(tweet)  # Suprime tous les caractères spéciaux, les RT et les pseudonymes\n",
    "            ListeTweets.append(tweetN)\n",
    "        Taille = len(ListeTweets)\n",
    "        Début = '{'+'\"'+'Tweets'+'\"'+':'+'['\n",
    "        ER1 = '\"'     # Utilisation d'expression régulière\n",
    "        ER2 = '\",'\n",
    "        \n",
    "        fd.write(Début)\n",
    "        for i in range(Taille-1):\n",
    "            ER3 = ER1 + ListeTweets[i] + ER2\n",
    "            fd.write(ER3)\n",
    "        ER4 = ER1 + ListeTweets[-1] + ER1\n",
    "        Fin = ']}'\n",
    "        fd.write(ER4+Fin)\n",
    "\n",
    "fichier = \"aitweets.json\"\n",
    "ZoneAtterrissage(fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des Données du fichier Json → DataFrame ('DFP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>AuthorLocation</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetLanguage</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>@</th>\n",
       "      <th>#</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1415291904850153474</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:39Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>@____bruvteresa_ According to research, NASA c...</td>\n",
       "      <td>[@____bruvteresa_]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1415291947560828933</td>\n",
       "      <td>Mysore  and  BERLIN</td>\n",
       "      <td>2021-07-14T12:47:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HDataSystems: Artificial Intelligence and ...</td>\n",
       "      <td>[@HDataSystems]</td>\n",
       "      <td>[#hdatasystems, #Artificia]</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1415291877897605120</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:33Z</td>\n",
       "      <td>246</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @adgpi: Army Technology Board conducted the...</td>\n",
       "      <td>[@adgpi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1415291886860967940</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @pacorjo: According to a recent survey, the...</td>\n",
       "      <td>[@pacorjo]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1415291968700264450</td>\n",
       "      <td>Internet</td>\n",
       "      <td>2021-07-14T12:47:54Z</td>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HarbRimah: Making AI Sing https://t.co/FJo...</td>\n",
       "      <td>[@HarbRimah]</td>\n",
       "      <td>[#MachineLearning, #DataScience, #Python, #AI,...</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>1421408743770791938</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>2021-07-31T09:53:47Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @innomaticshyd: Top 10 Real world Artificia...</td>\n",
       "      <td>[@innomaticshyd]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>1421424066305605634</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-31T10:54:40Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Iowa State part of U.S. National Science Found...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#ArtificialIntelligence, #IIoT, #GenerativeAd...</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>1421423882427371521</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>2021-07-31T10:53:57Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @intellimetri: Human Assisted #ArtificialIn...</td>\n",
       "      <td>[@intellimetri, @nerdgirlz, @forbes, @ForbesBR...</td>\n",
       "      <td>[#ArtificialIntelligence, #AI]</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>1421423971858149377</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "      <td>2021-07-31T10:54:18Z</td>\n",
       "      <td>12</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @IainLJBrown: Artificial Intelligence learn...</td>\n",
       "      <td>[@IainLJBrown]</td>\n",
       "      <td>[#Artificia]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>1421423964845395969</td>\n",
       "      <td>Torremolinos, España</td>\n",
       "      <td>2021-07-31T10:54:16Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @marcusborba: Artificial Intelligence May F...</td>\n",
       "      <td>[@marcusborba, @TAMU]</td>\n",
       "      <td>[#Healthcare, #A]</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1708 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id        AuthorLocation             CreatedAt  \\\n",
       "0     1415291904850153474                        2021-07-14T12:47:39Z   \n",
       "1     1415291947560828933   Mysore  and  BERLIN  2021-07-14T12:47:49Z   \n",
       "2     1415291877897605120                        2021-07-14T12:47:33Z   \n",
       "3     1415291886860967940                        2021-07-14T12:47:35Z   \n",
       "4     1415291968700264450              Internet  2021-07-14T12:47:54Z   \n",
       "...                   ...                   ...                   ...   \n",
       "1703  1421408743770791938    Liverpool, England  2021-07-31T09:53:47Z   \n",
       "1704  1421424066305605634                        2021-07-31T10:54:40Z   \n",
       "1705  1421423882427371521             127.0.0.1  2021-07-31T10:53:57Z   \n",
       "1706  1421423971858149377  Singapore, Singapore  2021-07-31T10:54:18Z   \n",
       "1707  1421423964845395969  Torremolinos, España  2021-07-31T10:54:16Z   \n",
       "\n",
       "     RetweetCount TweetLanguage  \\\n",
       "0               0            en   \n",
       "1               2            en   \n",
       "2             246            en   \n",
       "3               1            en   \n",
       "4              20            en   \n",
       "...           ...           ...   \n",
       "1703            1            en   \n",
       "1704            0            en   \n",
       "1705           17            en   \n",
       "1706           12            en   \n",
       "1707           17            en   \n",
       "\n",
       "                                              TweetText  \\\n",
       "0     @____bruvteresa_ According to research, NASA c...   \n",
       "1     RT @HDataSystems: Artificial Intelligence and ...   \n",
       "2     RT @adgpi: Army Technology Board conducted the...   \n",
       "3     RT @pacorjo: According to a recent survey, the...   \n",
       "4     RT @HarbRimah: Making AI Sing https://t.co/FJo...   \n",
       "...                                                 ...   \n",
       "1703  RT @innomaticshyd: Top 10 Real world Artificia...   \n",
       "1704  Iowa State part of U.S. National Science Found...   \n",
       "1705  RT @intellimetri: Human Assisted #ArtificialIn...   \n",
       "1706  RT @IainLJBrown: Artificial Intelligence learn...   \n",
       "1707  RT @marcusborba: Artificial Intelligence May F...   \n",
       "\n",
       "                                                      @  \\\n",
       "0                                    [@____bruvteresa_]   \n",
       "1                                       [@HDataSystems]   \n",
       "2                                              [@adgpi]   \n",
       "3                                            [@pacorjo]   \n",
       "4                                          [@HarbRimah]   \n",
       "...                                                 ...   \n",
       "1703                                   [@innomaticshyd]   \n",
       "1704                                                 []   \n",
       "1705  [@intellimetri, @nerdgirlz, @forbes, @ForbesBR...   \n",
       "1706                                     [@IainLJBrown]   \n",
       "1707                              [@marcusborba, @TAMU]   \n",
       "\n",
       "                                                      # Sentiment  \n",
       "0                                                    []   Positif  \n",
       "1                           [#hdatasystems, #Artificia]   Negatif  \n",
       "2                                                    []   Negatif  \n",
       "3                                                    []   Positif  \n",
       "4     [#MachineLearning, #DataScience, #Python, #AI,...    Neutre  \n",
       "...                                                 ...       ...  \n",
       "1703                                                 []   Positif  \n",
       "1704  [#ArtificialIntelligence, #IIoT, #GenerativeAd...   Positif  \n",
       "1705                     [#ArtificialIntelligence, #AI]    Neutre  \n",
       "1706                                       [#Artificia]   Positif  \n",
       "1707                                  [#Healthcare, #A]   Negatif  \n",
       "\n",
       "[1708 rows x 9 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RecupDonneesP(NomFichier=str):  # Entrée : 'str' \n",
    "    # Stockage et analyses préliminaires des données. L'objectif est ici d'élaborer le DataFrame principal sur lequel toutes nos analyses vont reposer.\n",
    "    from pandas import DataFrame\n",
    "    from pandas import concat\n",
    "    from json import loads\n",
    "    \n",
    "    with open(NomFichier, 'r') as fs:\n",
    "        ligne = fs.readline()\n",
    "        dicoL = loads(ligne) # On charge le dictionnaire contenu à la ligne\n",
    "        tweet = Tweet(dicoL[\"TweetText\"]) # On converti le tweet ('str') en 'Tweet'\n",
    "        dicoL[\"TweetText\"] = tweet \n",
    "        dicoL[\"@\"] = tweet.Arobases # On ajoute au dictionnnaire de la ligne la liste des '@' du tweet\n",
    "        dicoL[\"#\"] = tweet.Hashtags\n",
    "        dicoL[\"Sentiment\"] = tweet.Sentiment\n",
    "        DFF = DataFrame([dicoL])   # On initialise le DatFrame qui va stocker en local les données de la base\n",
    "        indice = 1  #Initialisation du compteur de ligne\n",
    "\n",
    "        for ligne in fs:\n",
    "            dicoL = loads(ligne)\n",
    "            tweet = Tweet(dicoL[\"TweetText\"])\n",
    "            dicoL[\"TweetText\"] = tweet\n",
    "            dicoL[\"@\"] = tweet.Arobases\n",
    "            dicoL[\"#\"] = tweet.Hashtags\n",
    "            dicoL[\"Sentiment\"] = tweet.Sentiment\n",
    "            DF2 = DataFrame([dicoL])\n",
    "            DFF = concat([DFF,DF2])\n",
    "            indice +=1   \n",
    "            \n",
    "    DFF.index = list(i for i in range(0,indice)) # On modifie l'index des lignes en s'appuyant sur la variable \"indice\" pour plus de praticité               \n",
    "    return DFF\n",
    "\n",
    "fichier = \"aitweets.json\"\n",
    "DFP = RecupDonneesP(fichier)   # DFP correspond au DataFrame principal, dont les colonnes sont 'id','TweetText', etc\n",
    "DFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionnaire des Hashtags ('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DicoHashtags(DF):   # Entrée : DataFrame (la colonne des tweets)\n",
    "    # Stockage des Hashtags et de leur nombre d'occurrence dans un dictionnaire\n",
    "    DicoH = {}\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "\n",
    "    for i in range(Taille[0]):\n",
    "        tweet = DF.iloc[i]\n",
    "        ListeHashtags = tweet.Hashtags \n",
    "\n",
    "        for h in ListeHashtags:\n",
    "            if h in DicoH:\n",
    "                nombre = DicoH[h]\n",
    "                nombre +=1\n",
    "                DicoH[h] = nombre\n",
    "            else:\n",
    "                DicoH[h] = 1\n",
    "                \n",
    "    del DicoH['#']    # Il y a 33 # incomplets dans la base de données, ils sont pris en compte par la fonction, on prend donc le soin de suprimer cette clé du dico           \n",
    "    return DicoH      # Renvoie un Dictionnaire contenant tous les Hashtags en clé et le nombre de fois qu'ils sont tweeté en valeur\n",
    "\n",
    "DicoH = DicoHashtags(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionnaire des Arobases @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DicoArobases(DF):   # Entrée : DataFrame (la colonne des tweets)\n",
    "    # Stockage des Arobases et de leur nombre d'occurrence dans un dictionnaire\n",
    "    DicoA = {}\n",
    "    Taille = DF.shape  # Tuple contenant (le nombre de ligne, le nombre de colonnes)\n",
    "    \n",
    "    for i in range(Taille[0]):\n",
    "        tweet = DF.iloc[i]\n",
    "        ListeArobases = tweet.Arobases\n",
    "        for h in ListeArobases:\n",
    "            if h in DicoA:\n",
    "                nombre = DicoA[h]\n",
    "                nombre +=1\n",
    "                DicoA[h] = nombre\n",
    "            else:\n",
    "                DicoA[h] = 1 \n",
    "    del DicoA['@']               \n",
    "    return DicoA      # Renvoie un Dictionnaire contenant tous les Hashtags en clé et le nombre de fois qu'ils sont tweeté en valeur\n",
    "\n",
    "DicoA = DicoArobases(DFP[\"TweetText\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionnaire des Topics\n",
    "La fonction ```Topics()``` étant beaucoup trop long à l'éxécution n'est pas connecté au reste. \n",
    "Mais celle ci aurait été encapsulée dans la classe ***Tweet*** et utilisée de la même manière que les fonctions ```Arobases()``` et ```Hashtags``` pour former un DicoT (qui contiendra tous les Topics et leur nombre d'apparition dans la base de données, exactement sur le même principe que pour les @ et les #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisateurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'enjeux est ici de compléter notre base de données qui est incomplète. En effet notre base de données comporte pour chaque tweet un champ **ID** mais aucun \"Pseudo\" ou \"Nom d'Utilisateur\".\n",
    "C'est pourquoi nous allons ajouter à notre **DataFrame** principal ce nouveau champ \"Utilisateurs\".\n",
    "\n",
    "La création d'Utilisateurs est essentielle au bon déroulement de certaines opérations d'analyses, notamment pour \"Les utilisateurs mentionnés par un utilisateur spécifique\". Il est donc impératif que tous les utilisateurs mentionnés dans les tweets soient présent au sein de la base de données, donc dans le champ \"**Utilisateurs**\" du DataFrame **DFP**. Mais il y a moins d'utilisateurs mentionnés que de tweets dans la base de données, c'est pourquoi nous allons compléter la base par des Nom d'Utilisateurs générés aléatoirement.\n",
    "\n",
    "En outre le champ utilisateur sera constitué avec les @ mentionnés au seins de la base de données ainsi que des @ créés de toute pièce et parfaitement unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utilisateurs</th>\n",
       "      <th>id</th>\n",
       "      <th>AuthorLocation</th>\n",
       "      <th>CreatedAt</th>\n",
       "      <th>RetweetCount</th>\n",
       "      <th>TweetLanguage</th>\n",
       "      <th>TweetText</th>\n",
       "      <th>@</th>\n",
       "      <th>#</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@TAMU</td>\n",
       "      <td>1415291904850153474</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:39Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>@____bruvteresa_ According to research, NASA c...</td>\n",
       "      <td>[@____bruvteresa_]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@inte</td>\n",
       "      <td>1415291947560828933</td>\n",
       "      <td>Mysore  and  BERLIN</td>\n",
       "      <td>2021-07-14T12:47:49Z</td>\n",
       "      <td>2</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HDataSystems: Artificial Intelligence and ...</td>\n",
       "      <td>[@HDataSystems]</td>\n",
       "      <td>[#hdatasystems, #Artificia]</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ForbesBR</td>\n",
       "      <td>1415291877897605120</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:33Z</td>\n",
       "      <td>246</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @adgpi: Army Technology Board conducted the...</td>\n",
       "      <td>[@adgpi]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@forbes</td>\n",
       "      <td>1415291886860967940</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-14T12:47:35Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @pacorjo: According to a recent survey, the...</td>\n",
       "      <td>[@pacorjo]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nerdgirlz</td>\n",
       "      <td>1415291968700264450</td>\n",
       "      <td>Internet</td>\n",
       "      <td>2021-07-14T12:47:54Z</td>\n",
       "      <td>20</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @HarbRimah: Making AI Sing https://t.co/FJo...</td>\n",
       "      <td>[@HarbRimah]</td>\n",
       "      <td>[#MachineLearning, #DataScience, #Python, #AI,...</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>@Kecv7</td>\n",
       "      <td>1421408743770791938</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>2021-07-31T09:53:47Z</td>\n",
       "      <td>1</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @innomaticshyd: Top 10 Real world Artificia...</td>\n",
       "      <td>[@innomaticshyd]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>@Gkqi4</td>\n",
       "      <td>1421424066305605634</td>\n",
       "      <td></td>\n",
       "      <td>2021-07-31T10:54:40Z</td>\n",
       "      <td>0</td>\n",
       "      <td>en</td>\n",
       "      <td>Iowa State part of U.S. National Science Found...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[#ArtificialIntelligence, #IIoT, #GenerativeAd...</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>@Znup0</td>\n",
       "      <td>1421423882427371521</td>\n",
       "      <td>127.0.0.1</td>\n",
       "      <td>2021-07-31T10:53:57Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @intellimetri: Human Assisted #ArtificialIn...</td>\n",
       "      <td>[@intellimetri, @nerdgirlz, @forbes, @ForbesBR...</td>\n",
       "      <td>[#ArtificialIntelligence, #AI]</td>\n",
       "      <td>Neutre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>@Dlxc3</td>\n",
       "      <td>1421423971858149377</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "      <td>2021-07-31T10:54:18Z</td>\n",
       "      <td>12</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @IainLJBrown: Artificial Intelligence learn...</td>\n",
       "      <td>[@IainLJBrown]</td>\n",
       "      <td>[#Artificia]</td>\n",
       "      <td>Positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>@Ekgt4</td>\n",
       "      <td>1421423964845395969</td>\n",
       "      <td>Torremolinos, España</td>\n",
       "      <td>2021-07-31T10:54:16Z</td>\n",
       "      <td>17</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @marcusborba: Artificial Intelligence May F...</td>\n",
       "      <td>[@marcusborba, @TAMU]</td>\n",
       "      <td>[#Healthcare, #A]</td>\n",
       "      <td>Negatif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1708 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Utilisateurs                   id        AuthorLocation  \\\n",
       "0           @TAMU  1415291904850153474                         \n",
       "1           @inte  1415291947560828933   Mysore  and  BERLIN   \n",
       "2       @ForbesBR  1415291877897605120                         \n",
       "3         @forbes  1415291886860967940                         \n",
       "4      @nerdgirlz  1415291968700264450              Internet   \n",
       "...           ...                  ...                   ...   \n",
       "1703       @Kecv7  1421408743770791938    Liverpool, England   \n",
       "1704       @Gkqi4  1421424066305605634                         \n",
       "1705       @Znup0  1421423882427371521             127.0.0.1   \n",
       "1706       @Dlxc3  1421423971858149377  Singapore, Singapore   \n",
       "1707       @Ekgt4  1421423964845395969  Torremolinos, España   \n",
       "\n",
       "                 CreatedAt RetweetCount TweetLanguage  \\\n",
       "0     2021-07-14T12:47:39Z            0            en   \n",
       "1     2021-07-14T12:47:49Z            2            en   \n",
       "2     2021-07-14T12:47:33Z          246            en   \n",
       "3     2021-07-14T12:47:35Z            1            en   \n",
       "4     2021-07-14T12:47:54Z           20            en   \n",
       "...                    ...          ...           ...   \n",
       "1703  2021-07-31T09:53:47Z            1            en   \n",
       "1704  2021-07-31T10:54:40Z            0            en   \n",
       "1705  2021-07-31T10:53:57Z           17            en   \n",
       "1706  2021-07-31T10:54:18Z           12            en   \n",
       "1707  2021-07-31T10:54:16Z           17            en   \n",
       "\n",
       "                                              TweetText  \\\n",
       "0     @____bruvteresa_ According to research, NASA c...   \n",
       "1     RT @HDataSystems: Artificial Intelligence and ...   \n",
       "2     RT @adgpi: Army Technology Board conducted the...   \n",
       "3     RT @pacorjo: According to a recent survey, the...   \n",
       "4     RT @HarbRimah: Making AI Sing https://t.co/FJo...   \n",
       "...                                                 ...   \n",
       "1703  RT @innomaticshyd: Top 10 Real world Artificia...   \n",
       "1704  Iowa State part of U.S. National Science Found...   \n",
       "1705  RT @intellimetri: Human Assisted #ArtificialIn...   \n",
       "1706  RT @IainLJBrown: Artificial Intelligence learn...   \n",
       "1707  RT @marcusborba: Artificial Intelligence May F...   \n",
       "\n",
       "                                                      @  \\\n",
       "0                                    [@____bruvteresa_]   \n",
       "1                                       [@HDataSystems]   \n",
       "2                                              [@adgpi]   \n",
       "3                                            [@pacorjo]   \n",
       "4                                          [@HarbRimah]   \n",
       "...                                                 ...   \n",
       "1703                                   [@innomaticshyd]   \n",
       "1704                                                 []   \n",
       "1705  [@intellimetri, @nerdgirlz, @forbes, @ForbesBR...   \n",
       "1706                                     [@IainLJBrown]   \n",
       "1707                              [@marcusborba, @TAMU]   \n",
       "\n",
       "                                                      # Sentiment  \n",
       "0                                                    []   Positif  \n",
       "1                           [#hdatasystems, #Artificia]   Negatif  \n",
       "2                                                    []   Negatif  \n",
       "3                                                    []   Positif  \n",
       "4     [#MachineLearning, #DataScience, #Python, #AI,...    Neutre  \n",
       "...                                                 ...       ...  \n",
       "1703                                                 []   Positif  \n",
       "1704  [#ArtificialIntelligence, #IIoT, #GenerativeAd...   Positif  \n",
       "1705                     [#ArtificialIntelligence, #AI]    Neutre  \n",
       "1706                                       [#Artificia]   Positif  \n",
       "1707                                  [#Healthcare, #A]   Negatif  \n",
       "\n",
       "[1708 rows x 10 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CreationUtilisateurs(ListeD, nb=int, DFP=DataFrame):  # Entrée : Liste Utilisateurs mentionnés ('dict_keys'), nombre ('int'), DataFrame principal (DFP)\n",
    "    # Ajout d'un nouveau champ \"Utilisateurs\" au DataFrame principal\n",
    "    from random import randint\n",
    "    from pandas import DataFrame\n",
    "    from pandas import concat\n",
    "\n",
    "    ListeUtilisateurs = list(ListeD)\n",
    "    ListeUtilisateurs.reverse()    # On inverse la liste des utilisateurs mentionnés pour éviter qu'un utilisateur se mentitonne lui même\n",
    "    for i in range(nb):\n",
    "        Lettre1 = chr(randint(65, 90))   # Intervalle Unicode des lettres Majuscules de l'alphabet latin\n",
    "        Lettre2 = chr(randint(97, 122))  \n",
    "        Lettre3 = chr(randint(97, 122))  # Intervalle Unicode des lettres Minuscules de l'alphabet latin\n",
    "        Lettre4 = chr(randint(97, 122))\n",
    "        Chiffre1 = chr(randint(48, 57))  # Intervalle Unicode des chiffres \n",
    "        Utilisateur = '@' + Lettre1 + Lettre2 + Lettre3 + Lettre4 + Chiffre1  # On ajoute '@' en début de Nom d'utilisateur pour s'assurer de l'homogéité des données.\n",
    "        ListeUtilisateurs.append(Utilisateur)\n",
    "        \n",
    "    DFUtilisateurs = DataFrame({\"Utilisateurs\": ListeUtilisateurs})\n",
    "    DFP = concat([DFUtilisateurs, DFP], axis=1)\n",
    "    return DFP  # Sortie : DataFrame (le DataFrame principal modifié, contenant le nouveau champ \"UTilisateurs\")\n",
    "   \n",
    "DFP = CreationUtilisateurs(DicoA.keys(), (1708-len(DicoA)), DFP)\n",
    "DFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilisation de la classe ***Inpoda***\n",
    "Maintenant que les trois variables ***DicoA***, ***DicoH*** et ***DFP*** ont été complétées on peut les transformer en objet **'Inpoda'** pour réaliser les opérations d'analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFP = Inpoda(DF= DFP)\n",
    "DicoH = Inpoda(Dico= DicoH)\n",
    "DicoA = Inpoda(Dico= DicoA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarques : \n",
    "- Les variables ***DicoA***, ***DicoH*** et ***DFP*** sont maintenant des objets **'Inpoda'** mais on peut toujours accéder à leurs structures de données initiales respectivement à l'aide des attributs ```'.Dico'``` et ```'.DF'```.\n",
    "- Les fonctionnalitées d'analyses (TopK Hashtags, etc) demandées sont accessibles pour tous les objets **'Inpoda'**. (Des exemples seront mit ci-dessous dans des cellules python sous forme de commentaires pour vous donner un aperçu des possibilitées) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctionnalitées Suplémentaires :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Actualisation manuelle des données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Refresh():\n",
    "    # Réactualisation de la Zone d'Atterrissage\n",
    "    fichier = \"aitweets.json\"\n",
    "    ZoneAtterrissage(fichier)\n",
    "\n",
    "Refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les TOP-K \n",
    "#### Accédez au Top décroissant avec K>0\n",
    "#### Et au Top croissant avec K<0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top K hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Nombre de Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>#ArtificialIntelligence</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#AI</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#MachineLearning</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>#ai</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#DataScience</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          #  Nombre de Tweets\n",
       "11  #ArtificialIntelligence               207\n",
       "5                       #AI               204\n",
       "2          #MachineLearning                86\n",
       "27                      #ai                52\n",
       "3              #DataScience                50"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DicoH.TopKH(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autre Exemple :\n",
    "#DFP.TopKH(K=5,DicoH=DicoH.Dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top K Utilisateurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utilisateurs</th>\n",
       "      <th>RetweetCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>@Ccfz5</td>\n",
       "      <td>10327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>@Reww6</td>\n",
       "      <td>6481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>@Hrqw1</td>\n",
       "      <td>6481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>@LuxuryOpinions</td>\n",
       "      <td>1660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>@SecDef</td>\n",
       "      <td>1060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Utilisateurs RetweetCount\n",
       "1194           @Ccfz5        10327\n",
       "1585           @Reww6         6481\n",
       "1587           @Hrqw1         6481\n",
       "735   @LuxuryOpinions         1660\n",
       "1047          @SecDef         1060"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFP.TopKU(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autre exemple :\n",
    "#DicoA.TopKU(-8, DFP.DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top K utilisateurs mentionnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@</th>\n",
       "      <th>Nombre de Mentions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@IainLJBrown</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>@Paula_Piccard</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@nigewillson</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>@JoshuaBarbeau</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>@sankrant</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  @  Nombre de Mentions\n",
       "26     @IainLJBrown                 112\n",
       "55   @Paula_Piccard                  49\n",
       "18     @nigewillson                  24\n",
       "679  @JoshuaBarbeau                  17\n",
       "977       @sankrant                  17"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DicoA.TopKA(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autre Exemple :\n",
    "#DFP.TopKA(K=-6,DicoA=DicoA.Dico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le Nombre de publication par Utilisateur/Hashtag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le nombre de publications par utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('@adgpi', 27)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFP.NbPostUtilisateur(Utilisateur=\"@adgpi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le nombre de publications par hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('#AI', 204)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DicoH.NbPostHashtags(hashtag= '#AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les dernières analyses demandées"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L’ensemble de tweets d’un utilisateur spécifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets postés par : @Gizchina</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OPPO releases its first 6G white paper. A syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @sfchronicle: “Intellectually, I know it’s ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Tweets postés par : @Gizchina\n",
       "0  OPPO releases its first 6G white paper. A syst...\n",
       "1  RT @sfchronicle: “Intellectually, I know it’s ..."
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFP.TweetsUtilisateur(Utilisateur=\"@Gizchina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L’ensemble de tweets mentionnant un utilisateur spécifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets mentionnant : @intratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>RT @intratio: $MDVL https://t.co/rso4WTkGcW Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>RT @intratio: https://t.co/GGI5g1FRD3 Kinnate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>RT @intratio: $CUE https://t.co/N3KHKWvbdp Cue...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Tweets mentionnant : @intratio\n",
       "75   RT @intratio: $MDVL https://t.co/rso4WTkGcW Me...\n",
       "400  RT @intratio: https://t.co/GGI5g1FRD3 Kinnate ...\n",
       "843  RT @intratio: $CUE https://t.co/N3KHKWvbdp Cue..."
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFP.TweetsMentionUtilisateur(Utilisateur=\"@intratio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les utilisateurs mentionnant un hashtag spécifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utilisateurs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@nerdgirlz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@arXiv_Daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@DecisionsSmart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>@frontiersin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>@TACHOUHONER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>@Nnfh4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>@Yasx5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>@Jell4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>@Qfpc6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705</th>\n",
       "      <td>@Znup0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Utilisateurs\n",
       "4          @nerdgirlz\n",
       "8        @arXiv_Daily\n",
       "10    @DecisionsSmart\n",
       "42       @frontiersin\n",
       "58       @TACHOUHONER\n",
       "...               ...\n",
       "1679           @Nnfh4\n",
       "1681           @Yasx5\n",
       "1684           @Jell4\n",
       "1699           @Qfpc6\n",
       "1705           @Znup0\n",
       "\n",
       "[200 rows x 1 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFP.UtilisateurMentionHashtag(Hashtag='#AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les utilisateurs mentionnés par un utilisateur spécifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@xoce_q a mentionné :</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@codewithibrahim</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  @xoce_q a mentionné :\n",
       "0      @codewithibrahim"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DFP.UtilisateurMentionUtilsateur(Utilisateur=\"@xoce_q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interface Graphique de Inpoda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ressources suplémentaires\n",
    "Des ressources suplémentaire sont disponible sur le Github dont le lien est ci dessous. Vous y trouverez notamment un diagramme détaillant le fonctionnement de **InPoDa** ainsi que le détail de la répartition des taches.\n",
    "\n",
    "**Github** : https://github.com/LugolBis/Projet-IN304 \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
